# model.py
import torch
import re
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from collections import OrderedDict
import math

class GPT2PPL:
    def __init__(self, device="cpu"):
        self.device = device
        self.model_id = "gpt2"
        self.model = GPT2LMHeadModel.from_pretrained(self.model_id).to(device)  # Added self.
        self.tokenizer = GPT2TokenizerFast.from_pretrained(self.model_id)  # Added self.

        self.max_length = self.model.config.n_positions
        self.stride = 512
        
    def getResults(self, threshold):
        if not isinstance(threshold, (int, float)) or math.isnan(threshold):
            return "Unable to determine due to insufficient or invalid text.", -1
        if threshold < 60:
            label = 0
            return "The Text is generated by AI.", label
        elif threshold < 80:
            label = 0
            return "The Text is most probably contain parts which are generated by AI. (require more text for better Judgement)", label
        else:
            label = 1
            return "The Text is written by Human.", label

    def __call__(self, sentence):
        try:
            results = OrderedDict()

            total_valid_char = re.findall("[a-zA-Z0-9]+", sentence)
            total_valid_char = sum([len(x) for x in total_valid_char])

            if total_valid_char < 100:
                return {"status": "Please input more text (min 100 characters)"}, "Please input more text (min 100 characters)"
            
            lines = re.split(r'(?<=[.?!][ \[\(])|(?<=\n)\s*',sentence)
            lines = list(filter(lambda x: (x is not None) and (len(x) > 0), lines))

            try:
                ppl = self.getPPL(sentence)
                results["Perplexity"] = ppl if not math.isnan(ppl) else 0
            except Exception as e:
                print(f"Error calculating overall perplexity: {e}")
                results["Perplexity"] = 0

            offset = ""
            Perplexity_per_line = []
            for i, line in enumerate(lines):
                if re.search("[a-zA-Z0-9]+", line) == None:
                    continue
                if len(offset) > 0:
                    line = offset + line
                    offset = ""
                if line[0] == "\n" or line[0] == " ":
                    line = line[1:]
                if line[-1] == "\n" or line[-1] == " ":
                    line = line[:-1]
                elif line[-1] == "[" or line[-1] == "(":
                    offset = line[-1]
                    line = line[:-1]
                try:
                    ppl = self.getPPL(line)
                    if not math.isnan(ppl):
                        Perplexity_per_line.append(ppl)
                except Exception as e:
                    print(f"Error calculating perplexity for line {i}: {e}")
                    continue

            if Perplexity_per_line:
                avg_ppl = sum(Perplexity_per_line)/len(Perplexity_per_line)
                max_ppl = max(Perplexity_per_line)
            else:
                avg_ppl = 0
                max_ppl = 0

            results["Perplexity per line"] = avg_ppl
            results["Burstiness"] = max_ppl

            out, label = self.getResults(avg_ppl)
            results["label"] = label

            return results, out
        except Exception as e:
            print(f"Error in analysis: {e}")
            return {"status": "Error analyzing text"}, "Error analyzing text"

    def getPPL(self, sentence):
        try:
            encodings = self.tokenizer(sentence, return_tensors="pt")
            seq_len = encodings.input_ids.size(1)

            nlls = []
            prev_end_loc = 0
            for begin_loc in range(0, seq_len, self.stride):
                end_loc = min(begin_loc + self.max_length, seq_len)
                trg_len = end_loc - prev_end_loc
                input_ids = encodings.input_ids[:, begin_loc:end_loc].to(self.device)
                target_ids = input_ids.clone()
                target_ids[:, :-trg_len] = -100

                with torch.no_grad():
                    outputs = self.model(input_ids, labels=target_ids)
                    neg_log_likelihood = outputs.loss * trg_len

                nlls.append(neg_log_likelihood)
                prev_end_loc = end_loc
                if end_loc == seq_len:
                    break

            if not nlls:
                return 0

            final_nll = torch.stack(nlls).sum()
            if torch.isnan(final_nll) or end_loc == 0:
                return 0
                
            ppl = torch.exp(final_nll / end_loc)
            return int(ppl.item()) if not torch.isnan(ppl) else 0
        except Exception as e:
            print(f"Error in getPPL: {e}")
            return 0